Function profiling
==================
  Message: /home/guus/anaconda3/envs/pcntk_dev/lib/python3.12/site-packages/pymc/pytensorf.py:1023
  Time in 1000 calls to Function.__call__: 1.645095e+01s
  Time in Function.vm.__call__: 16.269696942945302s (98.898%)
  Time in thunks: 16.168827056884766s (98.285%)
  Total compilation time: 1.559917e+01s
    Number of Apply nodes: 64
    PyTensor rewrite time: 2.101425e+00s
       PyTensor validate time: 2.906214e-02s
    PyTensor Linker time (includes C, CUDA code generation/compiling): 13.482848918996751s
       C-cache preloading 1.143499e-02s
       Import time 1.744317e-02s
       Node make_thunk time 1.346880e+01s
           Node Composite{...}(AdvancedSubtensor1.0, y, AdvancedSubtensor1.0, Composite{...}.1, ExpandDims{axis=0}.0) time 2.995202e+00s
           Node Composite{...}(sigma_sigma_log__, 0.0, Sum{axes=None}.0, 0, mu_sigma) time 2.748567e+00s
           Node Composite{(((((cast{float32}(i8) / i7) + (i5 * i6)) * i4) / (i2 + i3)) - (i0 - i1))}(delta, 1.0, 1.0, DropDims{axis=0}.0, DropDims{axis=0}.0, 2.0, Sum{axes=None}.0, DropDims{axis=0}.0, Assert{msg=Could not broadcast dimensions. Broadcasting is only allowed along axes that have a statically known length 1. Use `specify_broadcastable` to inform PyTensor of a known shape.}.0) time 2.058165e+00s
           Node Composite{...}(ExpandDims{axis=0}.0) time 1.932129e+00s
           Node Join(0, SpecifyShape.0, SpecifyShape.0, Reshape{1}.0, SpecifyShape.0, SpecifyShape.0, (d__logp/doffset_intercept_mu), SpecifyShape.0, SpecifyShape.0, (d__logp/dsigma), SpecifyShape.0, SpecifyShape.0) time 1.839340e+00s

Time in all call to pytensor.grad() 9.728619e-01s
Time since pytensor import 117.204s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  82.0%    82.0%      13.252s       6.63e-04s     C    20000      20   pytensor.tensor.elemwise.Elemwise
  10.1%    92.0%       1.628s       8.14e-04s     C     2000       2   pytensor.tensor.subtensor.AdvancedIncSubtensor1
   5.4%    97.4%       0.868s       4.34e-04s     C     2000       2   pytensor.tensor.subtensor.AdvancedSubtensor1
   1.6%    99.0%       0.264s       4.40e-05s     C     6000       6   pytensor.tensor.math.Sum
   0.4%    99.5%       0.072s       7.99e-06s     C     9000       9   pytensor.tensor.shape.Reshape
   0.3%    99.8%       0.045s       5.63e-06s     C     8000       8   pytensor.tensor.elemwise.DimShuffle
   0.1%    99.9%       0.021s       2.09e-05s     C     1000       1   pytensor.tensor.basic.Join
   0.0%    99.9%       0.006s       7.33e-07s     C     8000       8   pytensor.tensor.shape.SpecifyShape
   0.0%   100.0%       0.005s       1.27e-06s     C     4000       4   pytensor.tensor.shape.Shape_i
   0.0%   100.0%       0.004s       4.40e-06s     C     1000       1   pytensor.tensor.basic.Alloc
   0.0%   100.0%       0.004s       1.17e-06s     C     3000       3   pytensor.raise_op.Assert
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  81.7%    81.7%      13.203s       1.32e-02s     C     1000        1   Composite{...}
  10.1%    91.7%       1.628s       8.14e-04s     C     2000        2   AdvancedIncSubtensor1{inplace,inc}
   5.4%    97.1%       0.868s       4.34e-04s     C     2000        2   AdvancedSubtensor1
   1.6%    98.7%       0.264s       4.40e-05s     C     6000        6   Sum{axes=None}
   0.4%    99.2%       0.072s       7.99e-06s     C     9000        9   Reshape{1}
   0.2%    99.4%       0.033s       5.58e-06s     C     6000        6   ExpandDims{axis=0}
   0.1%    99.5%       0.021s       2.09e-05s     C     1000        1   Join
   0.1%    99.6%       0.012s       5.79e-06s     C     2000        2   DropDims{axis=0}
   0.0%    99.6%       0.007s       6.56e-06s     C     1000        1   Composite{...}
   0.0%    99.7%       0.006s       7.33e-07s     C     8000        8   SpecifyShape
   0.0%    99.7%       0.006s       2.82e-06s     C     2000        2   Mul
   0.0%    99.7%       0.005s       1.27e-06s     C     4000        4   Shape_i{0}
   0.0%    99.8%       0.004s       4.40e-06s     C     1000        1   Alloc
   0.0%    99.8%       0.004s       3.93e-06s     C     1000        1   Composite{...}
   0.0%    99.8%       0.004s       3.73e-06s     C     1000        1   Composite{...}
   0.0%    99.8%       0.004s       1.17e-06s     C     3000        3   Assert{msg=Could not broadcast dimensions. Broadcasting is only allowed along axes that have a statically known length 1. Use `specify_broadcastable` to inform PyTensor of a known shape.}
   0.0%    99.8%       0.003s       1.05e-06s     C     3000        3   Eq
   0.0%    99.9%       0.003s       2.94e-06s     C     1000        1   Composite{...}
   0.0%    99.9%       0.003s       2.92e-06s     C     1000        1   Composite{(switch(ge(exp(i0), i3), (exp((i0 + i0)) * i1), i2) + i4)}
   0.0%    99.9%       0.003s       2.87e-06s     C     1000        1   Composite{((i0 * i1) + i2)}
   ... (remaining 8 Ops account for   0.10%(0.02s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  81.7%    81.7%      13.203s       1.32e-02s   1000    32   Composite{...}(AdvancedSubtensor1.0, y, AdvancedSubtensor1.0, Composite{...}.1, ExpandDims{axis=0}.0)
   5.2%    86.9%       0.841s       8.41e-04s   1000    38   AdvancedIncSubtensor1{inplace,inc}(Alloc.0, Composite{...}.3, batch_effect_0_data)
   4.9%    91.7%       0.787s       7.87e-04s   1000    45   AdvancedIncSubtensor1{inplace,inc}(Composite{...}.1, Composite{...}.2, batch_effect_0_data)
   2.8%    94.5%       0.448s       4.48e-04s   1000     7   AdvancedSubtensor1(sigma, batch_effect_0_data)
   2.6%    97.1%       0.420s       4.20e-04s   1000    29   AdvancedSubtensor1(Composite{(i2 + (i0 * i1))}.0, batch_effect_0_data)
   0.8%    97.9%       0.125s       1.25e-04s   1000    37   Sum{axes=None}(Composite{...}.1)
   0.8%    98.6%       0.122s       1.22e-04s   1000    36   Sum{axes=None}(Composite{...}.0)
   0.2%    98.8%       0.031s       3.07e-05s   1000    55   Reshape{1}((d__logp/dmu_intercept_mu), [-1])
   0.1%    98.9%       0.021s       2.09e-05s   1000    63   Join(0, SpecifyShape.0, SpecifyShape.0, Reshape{1}.0, SpecifyShape.0, SpecifyShape.0, (d__logp/doffset_intercept_mu), SpecifyShape.0, SpecifyShape.0, (d__logp/dsigma), SpecifyShape.0, SpecifyShape.0)
   0.1%    99.0%       0.012s       1.24e-05s   1000    43   Sum{axes=None}(AdvancedIncSubtensor1{inplace,inc}.0)
   0.1%    99.1%       0.011s       1.05e-05s   1000    49   Reshape{1}((d__logp/depsilon), [-1])
   0.1%    99.1%       0.009s       9.43e-06s   1000    20   Reshape{1}((d__logp/dmu_slope_mu), [-1])
   0.1%    99.2%       0.008s       8.44e-06s   1000    16   ExpandDims{axis=0}(Composite{...}.0)
   0.0%    99.2%       0.007s       6.92e-06s   1000     9   ExpandDims{axis=0}(mu_sigma)
   0.0%    99.3%       0.007s       6.56e-06s   1000    30   Composite{...}(sigma_sigma_log__, 0.0, Sum{axes=None}.0, 0, mu_sigma)
   0.0%    99.3%       0.007s       6.51e-06s   1000     2   ExpandDims{axis=0}(delta)
   0.0%    99.4%       0.006s       5.97e-06s   1000    22   DropDims{axis=0}(Composite{...}.1)
   0.0%    99.4%       0.006s       5.61e-06s   1000    23   DropDims{axis=0}(Composite{...}.0)
   0.0%    99.4%       0.005s       4.55e-06s   1000    33   ExpandDims{axis=0}(Composite{...}.0)
   0.0%    99.4%       0.004s       4.40e-06s   1000    15   Alloc([0.], Shape_i{0}.0)
   ... (remaining 44 Apply instances account for 0.55%(0.09s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the PyTensor flag lib__amblibm=True. This speeds up only some Elemwise operation.
  - With the default gcc libm, exp in float32 is slower than in float64! Try PyTensor flag floatX=float64, or install amdlibm and set the pytensor flags lib__amblibm=True
