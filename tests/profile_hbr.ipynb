{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['PYTENSOR_FLAGS'] = 'floatX=float32,config.profile=True,config.profile_memory=True'\n",
    "import numpy as np\n",
    "\n",
    "from pcntoolkit.normative_model.norm_utils import norm_init\n",
    "from pcntoolkit.util.utils import simulate_data\n",
    "import matplotlib.pyplot as plt\n",
    "from pcntoolkit.normative import estimate\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import cProfile\n",
    "import timeit\n",
    "import pickle\n",
    "from pytensor.compile.profiling import ProfileStats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "########################### Experiment Settings ###############################\n",
    "\n",
    "\n",
    "random_state = 29\n",
    "\n",
    "working_dir = '/home/guus/tmp'  # Specify a working directory to save data and results.\n",
    "\n",
    "simulation_method = 'linear'\n",
    "n_features = 1      # The number of input features of X\n",
    "n_grps = 10          # Number of batches in data\n",
    "n_samples = 2500     # Number of samples in each group (use a list for different\n",
    "# sample numbers across different batches)\n",
    "\n",
    "model_type = 'bspline' #  modelto try 'linear, ''polynomial', 'bspline'   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ############################## Data Simulation ################################\n",
    "\n",
    "\n",
    "# X_train, Y_train, grp_id_train, X_test, Y_test, grp_id_test, coef = \\\n",
    "#     simulate_data(simulation_method, n_samples, n_features, n_grps,\n",
    "#                   working_dir=working_dir, plot=True, noise='heteroscedastic_nongaussian', \n",
    "#                   random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sex_mask = np.random.choice([0, 1], size=X_train.shape[0])\n",
    "# Y_train[train_sex_mask == 0] += 0.2\n",
    "# grp_id_train = np.stack([grp_id_train[:,0], train_sex_mask],axis=1)\n",
    "# print(grp_id_train.shape)\n",
    "\n",
    "\n",
    "# test_sex_mask = np.random.choice([0, 1], size=X_test.shape[0])\n",
    "# Y_test[test_sex_mask == 0] += 0.2\n",
    "# grp_id_test = np.stack([grp_id_test[:,0], test_sex_mask],axis=1)\n",
    "# print(grp_id_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savedir = \"/home/guus/Desktop/pcn_profile_data\"\n",
    "# with open(os.path.join(savedir, 'train_data'), 'wb') as f:\n",
    "#     pickle.dump((X_train, Y_train, grp_id_train), f)\n",
    "# with open(os.path.join(savedir, 'test_data'), 'wb') as f:\n",
    "#     pickle.dump((X_test, Y_test, grp_id_test), f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = \"/home/guus/Desktop/pcn_profile_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data():\n",
    "    import pickle\n",
    "    with open(os.path.join(savedir, 'train_data'), 'rb') as f:\n",
    "        X_train, Y_train, grp_id_train = pickle.load(f)\n",
    "    with open(os.path.join(savedir, 'test_data'), 'rb') as f:\n",
    "        X_test, Y_test, grp_id_test = pickle.load(f)\n",
    "    return X_train, Y_train, grp_id_train, X_test, Y_test, grp_id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, grp_id_train, X_test, Y_test, grp_id_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: M1\n",
      "Initializing the model\n",
      "Getting the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [slope_mu, intercept_mu, sigma]\n",
      "Sampling 1 chain for 500 tune and 156 draw iterations (500 + 156 draws total) took 267 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: M2\n",
      "Initializing the model\n",
      "Getting the model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################# Fittig and Predicting ###############################\n",
    "# model_confs = {\"M4\":{\"random_intercept_mu\":'True',\"likelihood\":\"SHASHb\"}}\n",
    "\n",
    "model_confs = {\"M1\":{\"random_intercept_mu\":'False',\"likelihood\":\"Normal\"},\n",
    "               \"M2\":{\"random_intercept_mu\":'True',\"likelihood\":\"Normal\"},\n",
    "               \"M3\":{\"random_intercept_mu\":'True',\"likelihood\":\"SHASHo\"},\n",
    "               \"M4\":{\"random_intercept_mu\":'True',\"likelihood\":\"SHASHb\"}}\n",
    "\n",
    "for model_name, model_conf in model_confs.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"Initializing the model\")\n",
    "    nm = norm_init(X_train, Y_train, \n",
    "               alg='hbr',\n",
    "               model_type=model_type,\n",
    "               linear_mu='True',\n",
    "               random_slope_mu='False',\n",
    "               random_sigma='False',\n",
    "               linear_sigma='False',\n",
    "               linear_epsilon='False',\n",
    "               linear_delta='False',\n",
    "               **model_conf)\n",
    "    \n",
    "    os.makedirs(os.path.join(savedir, 'v30', model_name), exist_ok=True)\n",
    "\n",
    "    print(\"Getting the model\")\n",
    "    model = nm.hbr.get_model(X_train, Y_train, grp_id_train)\n",
    "    with model:\n",
    "        pm.sample(1000, tune=500, chains=1, cores=1, compute_convergence_checks=False, progressbar=False)\n",
    "\n",
    "    # print(\"Profiling logp\")\n",
    "    # logp_profile = model.profile(model.logp(), point=point)\n",
    "    # print(\"Saving profile as text\")\n",
    "    # with open(os.path.join(savedir, 'v30', model_name, 'logp_profile.txt'), 'wt') as a:\n",
    "    #     logp_profile.summary(a)\n",
    "    # print(\"Saving profile as pickle\")\n",
    "    # with open(os.path.join(savedir, 'v30', model_name, 'logp_profile.pkl'), 'wb') as a:\n",
    "    #     pickle.dump(logp_profile, a)\n",
    "\n",
    "    # print(\"Profiling gradient\")\n",
    "    # grad_profile = model.profile(pm.gradient(model.logp()), point=point) \n",
    "    # print(\"Saving profile as text\")\n",
    "    # with open(os.path.join(savedir, 'v30', model_name, 'grad_profile.txt'), 'wt') as a:\n",
    "    #     grad_profile.summary(a)\n",
    "    # print(\"Saving profile as pickle\")\n",
    "    # with open(os.path.join(savedir, 'v30', model_name, 'grad_profile.pkl'), 'wb') as a:\n",
    "    #     pickle.dump(grad_profile, a)\n",
    "        \n",
    "    # print(\"Saving visualization of logp\")\n",
    "    # pytensor.printing.pydotprint(model.logp(), os.path.join(savedir,'v30', model_name, 'logp.png'), var_with_name_simple=True)\n",
    "    # print(\"Saving visualization of logp gradient\")\n",
    "    # pytensor.printing.pydotprint(pm.gradient(model.logp()), os.path.join(savedir,'v30', model_name, 'logp_grad.png'), var_with_name_simple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_max_of(thing):\n",
    "    the_max = max(thing.items(), key=lambda x: x[1])\n",
    "    return str(the_max[0]), the_max[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data:ProfileStats):\n",
    "    data_dict = {}\n",
    "    data_dict['compile_time'] = data.compile_time\n",
    "    data_dict['max_op_time_name'],data_dict['max_op_time'] = get_string_max_of(data.op_time())\n",
    "    data_dict['max_class_time_name'],data_dict['max_class_time'] =   get_string_max_of(data.class_time())\n",
    "    data_dict['max_apply_time_name'],data_dict['max_apply_time'] =  get_string_max_of(data.apply_time)\n",
    "    data_dict['max_compute_total_times_name'],data_dict['max_compute_total_times'] = get_string_max_of(data.compute_total_times())\n",
    "    data_dict['nb_nodes'] = data.nb_nodes\n",
    "    data_dict['fct_call_time']  = data.fct_call_time\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: M1\n",
      "Model: M2\n",
      "Model: M3\n",
      "Model: M4\n"
     ]
    }
   ],
   "source": [
    "# Load the profiles\n",
    "model_confs = {\"M1\":{\"random_intercept_mu\":'False',\"likelihood\":\"Normal\"},\n",
    "               \"M2\":{\"random_intercept_mu\":'True',\"likelihood\":\"Normal\"},\n",
    "               \"M3\":{\"random_intercept_mu\":'True',\"likelihood\":\"SHASHo\"},\n",
    "               \"M4\":{\"random_intercept_mu\":'True',\"likelihood\":\"SHASHb\"}}\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for model_name, model_conf in model_confs.items():\n",
    "\n",
    "    with open(os.path.join(savedir, 'v30', model_name, 'logp_profile.pkl'), 'rb') as a:\n",
    "        logp_profile = pickle.load(a)\n",
    "\n",
    "    with open(os.path.join(savedir, 'v30', model_name, 'grad_profile.pkl'), 'rb') as a:\n",
    "        grad_profile = pickle.load(a)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    data_dict[('v0.30',model_name, 'logp_profile')] = extract_data(logp_profile)\n",
    "    data_dict[('v0.30',model_name, 'grad_profile')] = extract_data(grad_profile)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(savedir, 'v30', 'data_dict.pkl'), 'wb') as a:\n",
    "    pickle.dump(data_dict, a)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcntk_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
