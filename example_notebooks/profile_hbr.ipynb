{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBR transfer example\n",
    "\n",
    "Welcome to this example/tutorial notebook that will go through the fitting, evaluation, and transfering of HBR models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pcntoolkit as ptk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import pytensor as pt\n",
    "from pytensor.printing import pydotprint\n",
    "from pcntoolkit.dataio.norm_data import NormData\n",
    "from pcntoolkit.normative_model.norm_conf import NormConf\n",
    "from pcntoolkit.normative_model.norm_hbr import NormHBR\n",
    "from pcntoolkit.normative_model.norm_factory import load_normative_model\n",
    "from pcntoolkit.normative_model.norm_factory import create_normative_model\n",
    "from pcntoolkit.regression_model.hbr.hbr_conf import HBRConf\n",
    "from pcntoolkit.regression_model.hbr.param import Param\n",
    "import seaborn as sns\n",
    "from pytensor.compile.profiling import ProfileStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "savedir = \"/home/guus/Desktop/pcn_profile_data\"\n",
    "\n",
    "def load_data():\n",
    "    with open(os.path.join(savedir, 'train_data'), 'rb') as f:\n",
    "        X_train, Y_train, grp_id_train = pickle.load(f)\n",
    "    with open(os.path.join(savedir, 'test_data'), 'rb') as f:\n",
    "        X_test, Y_test, grp_id_test = pickle.load(f)\n",
    "    return X_train, Y_train, grp_id_train, X_test, Y_test, grp_id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, Y_train, grp_id_train, X_test, Y_test, grp_id_test = load_data()\n",
    "\n",
    "train_data = NormData.from_ndarrays(name=\"train\",\n",
    "                                    X=X_train,\n",
    "                                    y=Y_train,\n",
    "                                    batch_effects=grp_id_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration of normative model is valid.\n"
     ]
    }
   ],
   "source": [
    "# Create a NormConf object\n",
    "norm_conf = NormConf(savemodel=True, \n",
    "                    save_dir=\"resources/save_dir\", \n",
    "                    log_dir=\"resources/log_dir\",\n",
    "                    inscaler=\"standardize\",\n",
    "                    outscaler=\"standardize\",\n",
    "                    basis_function=\"bspline\",\n",
    "                    order=3,\n",
    "                    nknots=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the regression model\n",
    "\n",
    "HBR models need to specificy (possibly recursive) parameter configurations. Here, we configure a HBR model with a SHASHb likelihood, a bspline regression in `mu` and `sigma`, and a random effect in the intercept of `mu`. Note that because sigma has to be strictly positive, we specify a `softplus` mapping, so that the output of the linear regression is mapped to the positive domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: M1\n",
      "Constructing model\n",
      "Configuration of regression model is valid.\n",
      "Profiling logp\n",
      "Saving logp profile pickle\n",
      "Saving logp profile txt\n",
      "Profiling grad\n",
      "Saving grad profile pickle\n",
      "Saving grad profile txt\n",
      "Saving visualization of logp\n",
      "The output file is available at /home/guus/Desktop/pcn_profile_data/refactor/M1/logp.png\n",
      "Saving visualization of grad\n",
      "The output file is available at /home/guus/Desktop/pcn_profile_data/refactor/M1/logp_grad.png\n",
      "Model: M2\n",
      "Constructing model\n",
      "Configuration of regression model is valid.\n",
      "Profiling logp\n",
      "Saving logp profile pickle\n",
      "Saving logp profile txt\n",
      "Profiling grad\n",
      "Saving grad profile pickle\n",
      "Saving grad profile txt\n",
      "Saving visualization of logp\n",
      "The output file is available at /home/guus/Desktop/pcn_profile_data/refactor/M2/logp.png\n",
      "Saving visualization of grad\n",
      "The output file is available at /home/guus/Desktop/pcn_profile_data/refactor/M2/logp_grad.png\n",
      "Model: M3\n",
      "Constructing model\n",
      "Configuration of regression model is valid.\n",
      "Profiling logp\n",
      "Saving logp profile pickle\n",
      "Saving logp profile txt\n",
      "Profiling grad\n",
      "Saving grad profile pickle\n",
      "Saving grad profile txt\n",
      "Saving visualization of logp\n",
      "The output file is available at /home/guus/Desktop/pcn_profile_data/refactor/M3/logp.png\n",
      "Saving visualization of grad\n",
      "The output file is available at /home/guus/Desktop/pcn_profile_data/refactor/M3/logp_grad.png\n",
      "Model: M4\n",
      "Constructing model\n",
      "Configuration of regression model is valid.\n",
      "Profiling logp\n",
      "Saving logp profile pickle\n",
      "Saving logp profile txt\n",
      "Profiling grad\n",
      "Saving grad profile pickle\n",
      "Saving grad profile txt\n",
      "Saving visualization of logp\n",
      "The output file is available at /home/guus/Desktop/pcn_profile_data/refactor/M4/logp.png\n",
      "Saving visualization of grad\n",
      "The output file is available at /home/guus/Desktop/pcn_profile_data/refactor/M4/logp_grad.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_confs = {\"M1\":{\"random_intercept_mu\":False,\"likelihood\":\"Normal\"},\n",
    "               \"M2\":{\"random_intercept_mu\":True,\"likelihood\":\"Normal\"},\n",
    "               \"M3\":{\"random_intercept_mu\":True,\"likelihood\":\"SHASHo\"},\n",
    "               \"M4\":{\"random_intercept_mu\":True,\"likelihood\":\"SHASHb\"}}\n",
    "\n",
    "# model_confs = {               \"M3\":{\"random_intercept_mu\":True,\"likelihood\":\"SHASHo\"}, }\n",
    "\n",
    "\n",
    "# hbrdata = NormHBR.normdata_to_hbrdata(train_data)\n",
    "\n",
    "for model_name, model_conf in model_confs.items():\n",
    "    print(\"Model: \" + model_name)\n",
    "    print(\"Constructing model\")\n",
    "\n",
    "    hbr_conf = HBRConf.from_args({\"draws\": 1500, \"tune\": 500, \"chains\": 4, \"cores\": 16, \n",
    "                                  \"linear_mu\":True,\n",
    "                                  \"linear_sigma\":False,\n",
    "                                  \"linear_epsilon\":False,\n",
    "                                  \"linear_delta\":False,\n",
    "                                  \"random_slope_mu\":False,\n",
    "                                  \"random_intercept_sigma\":False,\n",
    "                                  \"random_slope_sigma\":False,\n",
    "                                  \"random_intercept_epsilon\":False,\n",
    "                                  \"random_slope_epsilon\":False,\n",
    "                                  \"random_intercept_delta\":False,\n",
    "                                  \"random_slope_delta\":False,\n",
    "                                  \"random_delta\":False,\n",
    "                                  \"random_epsilon\":False,\n",
    "                                  \"random_sigma\":False} | model_confs[model_name])\n",
    "    norm_model = NormHBR(norm_conf, hbr_conf)\n",
    "    norm_model.prepare(\"response_var_0\")\n",
    "    norm_model.preprocess(train_data)\n",
    "    hbrdata = norm_model.normdata_to_hbrdata(train_data)\n",
    "    norm_model.current_regression_model.create_pymc_graph(hbrdata)\n",
    "    norm_model.current_regression_model.pymc_model.to_graphviz(save=os.path.join(savedir, \"refactor\", model_name, \"model.png\"))\n",
    "\n",
    "    pymc_model = norm_model.current_regression_model.pymc_model\n",
    "    os.makedirs(os.path.join(savedir, \"refactor\", model_name), exist_ok=True)\n",
    "\n",
    "    print(\"Profiling logp\")\n",
    "    logp_profile = pymc_model.profile(pymc_model.logp())\n",
    "    print(\"Saving logp profile pickle\")\n",
    "    with open(os.path.join(savedir, \"refactor\", model_name, \"logp_profile.pkl\"), 'wb') as f:\n",
    "        pickle.dump(logp_profile, f)\n",
    "    print(\"Saving logp profile txt\")\n",
    "    with open(os.path.join(savedir, \"refactor\", model_name, \"logp_profile.txt\"), 'w') as f:\n",
    "        logp_profile.summary(f)\n",
    "\n",
    "    print(\"Profiling grad\")\n",
    "    grad_profile = pymc_model.profile(pm.gradient(pymc_model.logp()))\n",
    "    print(\"Saving grad profile pickle\")\n",
    "    with open(os.path.join(savedir, \"refactor\", model_name, \"grad_profile.pkl\"), 'wb') as f:\n",
    "        pickle.dump(grad_profile, f)\n",
    "    print(\"Saving grad profile txt\")\n",
    "    with open(os.path.join(savedir, \"refactor\", model_name, \"grad_profile.txt\"), 'w') as f:\n",
    "        grad_profile.summary(f)\n",
    "\n",
    "    print(\"Saving visualization of logp\")\n",
    "    pydotprint(pymc_model.logp(), os.path.join(savedir, \"refactor\", model_name, \"logp.png\"))\n",
    "\n",
    "    print(\"Saving visualization of grad\")\n",
    "    pydotprint(pm.gradient(pymc_model.logp()), os.path.join(savedir, \"refactor\", model_name, \"logp_grad.png\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_max_of(thing):\n",
    "    the_max = max(thing.items(), key=lambda x: x[1])\n",
    "    return str(the_max[0]), the_max[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data:ProfileStats):\n",
    "    data_dict = {}\n",
    "    data_dict['compile_time'] = data.compile_time\n",
    "    data_dict['max_op_time_name'],data_dict['max_op_time'] = get_string_max_of(data.op_time())\n",
    "    data_dict['max_class_time_name'],data_dict['max_class_time'] =   get_string_max_of(data.class_time())\n",
    "    data_dict['max_apply_time_name'],data_dict['max_apply_time'] =  get_string_max_of(data.apply_time)\n",
    "    data_dict['max_compute_total_times_name'],data_dict['max_compute_total_times'] = get_string_max_of(data.compute_total_times())\n",
    "    data_dict['nb_nodes'] = data.nb_nodes\n",
    "    data_dict['fct_call_time']  = data.fct_call_time\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: M1\n",
      "Model: M2\n",
      "Model: M3\n",
      "Model: M4\n"
     ]
    }
   ],
   "source": [
    "# Load the profiles\n",
    "model_confs = {\"M1\":{\"random_intercept_mu\":'False',\"likelihood\":\"Normal\"},\n",
    "               \"M2\":{\"random_intercept_mu\":'True',\"likelihood\":\"Normal\"},\n",
    "               \"M3\":{\"random_intercept_mu\":'True',\"likelihood\":\"SHASHo\"},\n",
    "               \"M4\":{\"random_intercept_mu\":'True',\"likelihood\":\"SHASHb\"}}\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for model_name, model_conf in model_confs.items():\n",
    "\n",
    "    with open(os.path.join(savedir, 'refactor', model_name, 'logp_profile.pkl'), 'rb') as a:\n",
    "        logp_profile = pickle.load(a)\n",
    "\n",
    "    with open(os.path.join(savedir, 'refactor', model_name, 'grad_profile.pkl'), 'rb') as a:\n",
    "        grad_profile = pickle.load(a)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    data_dict[('refactor',model_name, 'logp_profile')] = extract_data(logp_profile)\n",
    "    data_dict[('refactor',model_name, 'grad_profile')] = extract_data(grad_profile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(savedir, 'refactor', 'data_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
